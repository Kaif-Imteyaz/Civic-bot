{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install dependencies"
      ],
      "metadata": {
        "id": "4960wnzE0PS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y huggingface_hub\n",
        "!pip install -q unsloth\n",
        "!pip install -q datasets pandas openpyxl pyreadstat transformers accelerate\n",
        "!pip install -q --upgrade huggingface_hub\n",
        "!pip install -q unsloth_zoo"
      ],
      "metadata": {
        "id": "s0I289560JBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount Drive"
      ],
      "metadata": {
        "id": "7nJcMVSM0RVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset\n",
        "import gc\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "import transformers\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "id": "o2geGnjZ0Vk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Memory optimization"
      ],
      "metadata": {
        "id": "Zdyuop8229vO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 1024\n",
        "load_in_4bit = True\n",
        "dtype = torch.float16\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "J6W0cClx289W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load base model"
      ],
      "metadata": {
        "id": "dW6rvp5I3D75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Kai-99/qwen2-1.5-bihar-offices\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    token=\"hf_\",\n",
        "    dtype=dtype,\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "z5WUOvpK3Ekh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Office Parser"
      ],
      "metadata": {
        "id": "tVthAvzz3J4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Enhanced Office Parser (your v2 code)\n",
        "import re\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "class OfficeColumnParser:\n",
        "    def __init__(self):\n",
        "        self.short_forms = self.load_short_forms()\n",
        "        self.districts = self.load_districts()\n",
        "        self.aliases = self.load_aliases()\n",
        "\n",
        "    def load_short_forms(self):\n",
        "        return {\n",
        "            # Educational Institutions\n",
        "            'HS': 'HIGH SCHOOL', 'H.S': 'HIGH SCHOOL', 'H/S': 'HIGH SCHOOL',\n",
        "            'MS': 'MIDDLE SCHOOL', 'M.S': 'MIDDLE SCHOOL', 'M/S': 'MIDDLE SCHOOL',\n",
        "            'PS': 'PRIMARY SCHOOL', 'P.S': 'PRIMARY SCHOOL', 'P/S': 'PRIMARY SCHOOL',\n",
        "            'GMS': 'GOVERNMENT MIDDLE SCHOOL',\n",
        "            'UMS': 'UPGRADED MIDDLE SCHOOL',\n",
        "            'GPS': 'GOVERNMENT PRIMARY SCHOOL',\n",
        "            'GBS': 'GOVERNMENT BASIC SCHOOL',\n",
        "            'NPS': 'NAVSRIJIT PRIMARY SCHOOL',\n",
        "            'PG H/S': 'PRABHAT GYAN HIGH SCHOOL',\n",
        "            'HSC': 'HIGHER SECONDARY SCHOOL',\n",
        "            'OFFICE HM': 'OFFICE OF HEAD MASTER',\n",
        "\n",
        "            # Healthcare\n",
        "            'PHC': 'PRIMARY HEALTH CENTRE', 'P.H.C': 'PRIMARY HEALTH CENTRE',\n",
        "            'APHC': 'ADDITIONAL PRIMARY HEALTH CENTRE',\n",
        "            'SNCU': 'SPECIAL NEWBORN CARE UNIT', 'S.N.C.U': 'SPECIAL NEWBORN CARE UNIT',\n",
        "            'VIMS': 'VARDHMAN INSTITUTE OF MEDICAL SCIENCES',\n",
        "            'CMO': 'CHIEF MEDICAL OFFICER',\n",
        "            'JNKTMCH': 'JANNAYAK KARPOORI THAKUR MEDICAL COLLEGE AND HOSPITAL',\n",
        "\n",
        "            # Government Departments\n",
        "            'RWD': 'RURAL WORKS DEPARTMENT', 'R.W.D': 'RURAL WORKS DEPARTMENT',\n",
        "            'T.C DIV': 'TRANSMISSION CIVIL DIVISION',\n",
        "            'SDO': 'SUB-DIVISIONAL OFFICE',\n",
        "            'CDPO': 'CHILD DEVELOPMENT PROJECT OFFICER',\n",
        "            'D.V.B.D.C.O': 'DISTRICT VECTOR-BORNE DISEASE CONTROL OFFICER',\n",
        "            'ARCS': 'ASSISTANT REGISTRAR OF CO-OPERATIVE SOCIETIES',\n",
        "            'ESD': 'ELECTRICAL SUB-DIVISION',\n",
        "            'DEO': 'DISTRICT EDUCATION OFFICE',\n",
        "            'MIDIV': 'MINOR IRRIGATION DIVISION',\n",
        "            'WITI': \"WOMEN'S INDUSTRIAL TRAINING INSTITUTE\",\n",
        "            'L.A.E.O.': 'LOCAL AREA ENGINEERING ORGANISATION',\n",
        "            'T.C DIV.':'TIRHUT CANAL DIVISON',\n",
        "            'O/O AD PP': 'OFFICE OF THE ADDITIONAL DIRECTOR OF PUBLIC PROSECUTION',\n",
        "\n",
        "            # Police & Security\n",
        "            'PS': 'POLICE STATION',\n",
        "            'SSP': 'SENIOR SUPERINTENDENT OF POLICE OFFICE',\n",
        "            'SP': 'SUPERINTENDENT OF POLICE OFFICE',\n",
        "            'CTS': 'CONSTABLE TRAINING SCHOOL',\n",
        "            'BSAP': 'BIHAR SPECIAL ARMED POLICE', 'B.S.A.P': 'BIHAR SPECIAL ARMED POLICE',\n",
        "            'SCJ': 'SPECIAL CENTRAL JAIL',\n",
        "            'SRP': 'SUPERINTENDENT OF RAILWAY POLICE',\n",
        "            'CO, BSAP': 'COMMANDING OFFICER BIHAR SPECIAL ARMED POLICE',\n",
        "            'BMP': 'BIHAR MILLITARY POLICE',\n",
        "            'BSAP': 'BIHAR SPECIAL ARMED POLICE (WOMEN WING)',\n",
        "\n",
        "\n",
        "            # Power & Infrastructure\n",
        "            'ESS': 'ELECTRICAL SUB STATION',\n",
        "            'SBPDCL': 'SOUTH BIHAR POWER DISTRIBUTION COMPANY LTD',\n",
        "            'NBPDCL': 'NORTH BIHAR POWER DISTRIBUTION COMPANY LTD'\n",
        "        }\n",
        "\n",
        "    def load_districts(self):\n",
        "        return [\n",
        "            'Vaishali', 'Supaul', 'Siwan', 'Sitamarhi', 'Sheohar', 'Sheikhpura', 'Saran',\n",
        "            'Samastipur', 'Saharsa', 'Rohtas', 'Purvi Champaran', 'Purnia', 'Patna',\n",
        "            'Pashchim Champaran', 'Nawada', 'Nalanda', 'Muzaffarpur', 'Munger',\n",
        "            'Madhubani', 'Madhepura', 'Lakhisarai', 'Kishanganj', 'Khagaria', 'Katihar',\n",
        "            'Kaimur (Bhabua)', 'Jehanabad', 'Jamui', 'Gopalganj', 'Gaya', 'Darbhanga',\n",
        "            'Buxar', 'Bhojpur', 'Bhagalpur', 'Begusarai', 'Banka', 'Aurangabad', 'Arwal',\n",
        "            'Araria'\n",
        "        ]\n",
        "\n",
        "    def load_aliases(self):\n",
        "        return {\n",
        "            'HIGH SCHL': 'HIGH SCHOOL',\n",
        "            'H SCHOOL': 'HIGH SCHOOL',\n",
        "            'UCHYA VIDYALAYA': 'HIGH SCHOOL',\n",
        "            '+2 SCHOOL': 'HIGH SCHOOL',\n",
        "            'INTERMEDIATE SCHOOL': 'HIGH SCHOOL',\n",
        "            'PULSE TWO': 'HIGH SCHOOL',\n",
        "            '10+2 SCHOOL': 'HIGH SCHOOL',\n",
        "            'MID. SCHOOL': 'MIDDLE SCHOOL',\n",
        "            'MADHYA SCHOOL': 'MIDDLE SCHOOL',\n",
        "            'MADHYAMIK SCHOOL': 'MIDDLE SCHOOL',\n",
        "            'MADHYA VIDYALAYA': 'MIDDLE SCHOOL',\n",
        "            'PRATHMIK SCHOOL': 'PRIMARY SCHOOL',\n",
        "            'P SCHOOL': 'PRIMARY SCHOOL',\n",
        "            'PRI. SCHOOL': 'PRIMARY SCHOOL',\n",
        "            'KANYA SCHOOL': 'GIRLS SCHOOL',\n",
        "            'KANYA VIDYALAYA': 'GIRLS SCHOOL',\n",
        "            'UTKRAMIT MIDDLE SCHOOL': 'UPGRADED MIDDLE SCHOOL',\n",
        "            'PRIM. HEALTH': 'PRIMARY HEALTH CENTRE',\n",
        "            'POLICE SUPERINTENDENT OFFICE': 'SUPERINTENDENT OF POLICE OFFICE'\n",
        "        }\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        text = text.upper().strip()\n",
        "        text = re.sub(r'[^\\w\\s,]', '', text)\n",
        "        return re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "# Instantiate parser\n",
        "office_parser = OfficeColumnParser()"
      ],
      "metadata": {
        "id": "KMDp4rO03NK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Load"
      ],
      "metadata": {
        "id": "L_dK8PEN3N_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Enhanced Data Loader with Alpaca-style prompts\n",
        "class DataProcessor:\n",
        "    def __init__(self, parser):\n",
        "        self.parser = parser\n",
        "\n",
        "    def load_training_data(self):\n",
        "        \"\"\"Load training data from Excel file with Alpaca formatting\"\"\"\n",
        "        try:\n",
        "            # Load your training data\n",
        "            df = pd.read_excel(\"/content/drive/MyDrive/civicBot/llm_training_data_v2.xlsx\")\n",
        "            print(f\"Loaded training data: {len(df)} samples\")\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading training data: {e}\")\n",
        "            # Fallback to sample data\n",
        "            return pd.DataFrame({\n",
        "                'office': ['RURAL WORKS DEPARTMENT, WORKS DIVISION, MAHUA', 'ESD HAJIPUR'],\n",
        "                'office_name': ['RURAL WORKS DEPARTMENT, WORKS DIVISION', 'ESD'],\n",
        "                'office_location': ['MAHUA', 'HAJIPUR']\n",
        "            })\n",
        "\n",
        "    def create_alpaca_prompt(self, sample):\n",
        "        \"\"\"Create Alpaca-style prompt for training\"\"\"\n",
        "        instruction = \"Extract the office name and location from the following Bihar government office text. Return the result in JSON format with 'office_name' and 'office_location' fields.\"\n",
        "\n",
        "        office = sample['office']\n",
        "        office_name = sample.get('office_name', '')\n",
        "        office_location = sample.get('office_location', '')\n",
        "\n",
        "        if office_name and office_location:\n",
        "            output_json = {\n",
        "                \"office_name\": office_name,\n",
        "                \"office_location\": office_location\n",
        "            }\n",
        "            output = json.dumps(output_json, ensure_ascii=False)\n",
        "        else:\n",
        "            output = '{\"office_name\": \"UNKNOWN\", \"office_location\": \"UNKNOWN\"}'\n",
        "\n",
        "        return {\n",
        "            \"instruction\": instruction,\n",
        "            \"input\": office,\n",
        "            \"output\": output\n",
        "        }\n",
        "\n",
        "    def prepare_training_dataset(self):\n",
        "        \"\"\"Prepare dataset for training\"\"\"\n",
        "        df = self.load_training_data()\n",
        "\n",
        "        # Apply Alpaca formatting\n",
        "        formatted_data = [self.create_alpaca_prompt(row) for _, row in df.iterrows()]\n",
        "\n",
        "        # Convert to dataset\n",
        "        dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "        # Tokenize function\n",
        "        def tokenize_function(examples):\n",
        "            prompts = []\n",
        "            for i in range(len(examples['instruction'])):\n",
        "                instruction = examples['instruction'][i]\n",
        "                input_text = examples['input'][i]\n",
        "                output_text = examples['output'][i]\n",
        "\n",
        "                prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input_text}\n",
        "\n",
        "### Response:\n",
        "{output_text}\"\"\"\n",
        "                prompts.append(prompt)\n",
        "\n",
        "            return tokenizer(prompts, truncation=True, max_length=max_seq_length, padding=False)\n",
        "\n",
        "        # Tokenize dataset\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            batch_size=4,\n",
        "            remove_columns=dataset.column_names\n",
        "        )\n",
        "\n",
        "        print(f\"Prepared training dataset with {len(tokenized_dataset)} samples\")\n",
        "        return tokenized_dataset\n",
        "\n",
        "# Initialize data processor\n",
        "data_processor = DataProcessor(office_parser)"
      ],
      "metadata": {
        "id": "Ka8DAuQ13Qz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LoRa Setup"
      ],
      "metadata": {
        "id": "CfhwYl0F3Uz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=True,\n",
        "    random_state=3407,\n",
        "    max_seq_length=max_seq_length,\n",
        ")"
      ],
      "metadata": {
        "id": "wenPimuR3W7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Setup"
      ],
      "metadata": {
        "id": "nx4rHB0G3Y2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Prepare training data\n",
        "train_dataset = data_processor.prepare_training_dataset()\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen2-bihar-offices-v2\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=50,\n",
        "    max_steps=500,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    logging_steps=25,\n",
        "    optim=\"adamw_8bit\",\n",
        "    weight_decay=0.01,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    seed=3407,\n",
        "    report_to=\"none\",\n",
        "    save_steps=250,\n",
        "    save_total_limit=1,\n",
        ")"
      ],
      "metadata": {
        "id": "iry96lye3aiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trainning"
      ],
      "metadata": {
        "id": "Z4sEyAzq3efH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "# Clear memory before training\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"./qwen2-bihar-offices-v2\")\n",
        "tokenizer.save_pretrained(\"./qwen2-bihar-offices-v2\")\n",
        "print(\"Model saved locally\")"
      ],
      "metadata": {
        "id": "-8zy4R4j3jDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Push to Hugging Face Hub"
      ],
      "metadata": {
        "id": "mjh_deMO3nIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=\"hf_\")\n",
        "\n",
        "model.push_to_hub(\"Kai-99/qwen2-1.5-bihar-offices-v2\", token=\"hf_\")\n",
        "tokenizer.push_to_hub(\"Kai-99/qwen2-1.5-bihar-offices-v2\", token=\"hf_\")\n",
        "\n",
        "print(\"Model pushed to Hugging Face Hub!\")"
      ],
      "metadata": {
        "id": "KFrR7CO13lr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference Engine"
      ],
      "metadata": {
        "id": "rvha656V3pkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InferenceEngine:\n",
        "    def __init__(self, model_path=\"Kai-99/qwen2-1.5-bihar-offices-v2\"):\n",
        "        self.model_path = model_path\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Load the trained model\"\"\"\n",
        "        print(\"Loading model for inference...\")\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=self.model_path,\n",
        "            max_seq_length=1024,\n",
        "            load_in_4bit=True,\n",
        "            dtype=torch.float16,\n",
        "        )\n",
        "        print(\"Model loaded successfully!\")\n",
        "\n",
        "    def extract_office_info(self, text):\n",
        "        \"\"\"Enhanced extraction with JSON output\"\"\"\n",
        "        prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "Extract the office name and location from the following Bihar government office text. Return the result in JSON format with 'office_name' and 'office_location' fields.\n",
        "\n",
        "### Input:\n",
        "{text}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,\n",
        "                temperature=0.1,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract just the response part\n",
        "        if \"### Response:\" in response:\n",
        "            response = response.split(\"### Response:\")[1].strip()\n",
        "\n",
        "        # Try to parse JSON\n",
        "        try:\n",
        "            # Find JSON in response\n",
        "            start_idx = response.find('{')\n",
        "            end_idx = response.rfind('}') + 1\n",
        "            if start_idx != -1 and end_idx != 0:\n",
        "                json_str = response[start_idx:end_idx]\n",
        "                parsed = json.loads(json_str)\n",
        "                return parsed\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Fallback parsing\n",
        "        return self.fallback_parsing(response, text)\n",
        "\n",
        "    def fallback_parsing(self, response, original_text):\n",
        "        \"\"\"Fallback parsing when JSON fails\"\"\"\n",
        "        office_name = \"UNKNOWN\"\n",
        "        office_location = \"UNKNOWN\"\n",
        "\n",
        "        # Simple pattern matching\n",
        "        if \"office_name\" in response.lower():\n",
        "            try:\n",
        "                parts = response.split(\"office_name\")[1].split(\"office_location\")[0]\n",
        "                office_name = parts.split(\":\")[1].split(\",\")[0].strip(' \"\\'')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if \"office_location\" in response.lower():\n",
        "            try:\n",
        "                parts = response.split(\"office_location\")[1]\n",
        "                office_location = parts.split(\":\")[1].split(\"}\")[0].strip(' \"\\'')\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return {\n",
        "            \"office_name\": office_name if office_name != \"UNKNOWN\" else original_text,\n",
        "            \"office_location\": office_location\n",
        "        }\n",
        "\n",
        "    def batch_extract_office_info(self, texts, batch_size=4):\n",
        "        \"\"\"Batch processing for efficiency\"\"\"\n",
        "        results = []\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "            batch_results = []\n",
        "\n",
        "            for text in batch_texts:\n",
        "                try:\n",
        "                    result = self.extract_office_info(text)\n",
        "                    batch_results.append(result)\n",
        "                except Exception as e:\n",
        "                    batch_results.append({\n",
        "                        \"office_name\": text,\n",
        "                        \"office_location\": f\"ERROR: {str(e)}\"\n",
        "                    })\n",
        "\n",
        "            results.extend(batch_results)\n",
        "            print(f\"Processed {min(i+batch_size, len(texts))}/{len(texts)} samples\")\n",
        "\n",
        "            # Memory cleanup\n",
        "            if i % 20 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        return results\n",
        "\n",
        "# Initialize inference engine\n",
        "inference_engine = InferenceEngine()"
      ],
      "metadata": {
        "id": "7hB9_DC43s5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation"
      ],
      "metadata": {
        "id": "7kqyKMo333lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_with_samples():\n",
        "    \"\"\"Test the model with sample inputs\"\"\"\n",
        "    test_samples = [\n",
        "        \"RURAL WORKS DEPARTMENT, WORKS DIVISION, MAHUA\",\n",
        "        \"ESD HAJIPUR\",\n",
        "        \"DISTRICT EDUCATION OFFICE, PATNA\",\n",
        "        \"PHC MADHEPURA\",\n",
        "        \"PS SAMASTIPUR\"\n",
        "    ]\n",
        "\n",
        "    print(\"Testing with sample data:\\n\")\n",
        "    for sample in test_samples:\n",
        "        result = inference_engine.extract_office_info(sample)\n",
        "        print(f\"Input: {sample}\")\n",
        "        print(f\"Output: {result}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "test_with_samples()"
      ],
      "metadata": {
        "id": "XuK3_L-s35V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run"
      ],
      "metadata": {
        "id": "cp_8vrAO3x3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_unseen_data():\n",
        "    \"\"\"Process 1k random samples from the .dta file\"\"\"\n",
        "    # Load .dta file\n",
        "    df = pd.read_stata(\"/content/drive/MyDrive/civicBot/_\")\n",
        "\n",
        "    # Sample 1k random entries\n",
        "    sample_df = df[['office', 'district', 'cadre', 'year']].sample(n=1000, random_state=42)\n",
        "\n",
        "    print(f\"Processing {len(sample_df)} samples...\")\n",
        "\n",
        "    # Extract office info\n",
        "    texts = sample_df['office'].tolist()\n",
        "    extracted_results = inference_engine.batch_extract_office_info(texts)\n",
        "\n",
        "    # Combine results\n",
        "    final_results = []\n",
        "    for i, (_, row) in enumerate(sample_df.iterrows()):\n",
        "        extracted = extracted_results[i]\n",
        "        final_results.append({\n",
        "            'office_original': row['office'],\n",
        "            'office_name': extracted.get('office_name', ''),\n",
        "            'office_location': extracted.get('office_location', ''),\n",
        "            'district': row['district'],\n",
        "            'cadre': row['cadre'],\n",
        "            'year': row['year']\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(final_results)\n",
        "\n",
        "# Run inference\n",
        "results_df = process_unseen_data()\n",
        "\n",
        "# Save to CSV\n",
        "results_df.to_csv(\"/content/drive/MyDrive/civicBot/A_inference_results_1k_v2.csv\", index=False)\n",
        "print(\"Results saved to inference_results_1k_v2.csv\")\n",
        "\n",
        "# Display sample results\n",
        "print(\"\\nSample results:\")\n",
        "print(results_df.head(10))"
      ],
      "metadata": {
        "id": "ct4wCPNW30LH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}